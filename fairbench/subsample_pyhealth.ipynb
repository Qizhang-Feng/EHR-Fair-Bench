{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from resample import subsample_subdataset\n",
    "import numpy as np\n",
    "from utils import evaluate, fair_check\n",
    "\n",
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "SENS_KEY = 'gender'\n",
    "root = '/data/qf31/EHR-Fair-Bench/fairbench/datasets/mimic-iii-clinical-database-1.4'\n",
    "mimic3base = MIMIC3Dataset(\n",
    "    root=root,\n",
    "    tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"], # PRESCRIPTIONS\n",
    "    # map all NDC codes to ATC 3-rd level codes in these tables\n",
    "    code_mapping={\"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}})},\n",
    "    dev=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for mortality_prediction_mimic3_fn: 100%|██████████| 5000/5000 [00:00<00:00, 90760.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.tasks import drug_recommendation_mimic3_fn, mortality_prediction_mimic3_fn, readmission_prediction_mimic3_fn\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "from tasks import add_feature\n",
    "#from tasks import drug_recommendation_mimic3_fn\n",
    "mimic3sample = mimic3base.set_task(task_fn=mortality_prediction_mimic3_fn) # use default task\n",
    "mimic3sample = add_feature(mimic3sample, mimic3base, [SENS_KEY])\n",
    "train_ds, val_ds, test_ds = split_by_patient(mimic3sample, [0.6, 0.2, 0.2])\n",
    "\n",
    "# create dataloaders (torch.data.DataLoader)\n",
    "train_loader = get_dataloader(train_ds, batch_size=3200, shuffle=True)\n",
    "val_loader = get_dataloader(val_ds, batch_size=3200, shuffle=False)\n",
    "test_loader = get_dataloader(test_ds, batch_size=3200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset',\n",
       " 'indices',\n",
       " '__module__',\n",
       " '__annotations__',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " '__getitem__',\n",
       " '__len__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__add__',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__slots__',\n",
       " '__new__',\n",
       " '__class_getitem__',\n",
       " '__init_subclass__',\n",
       " '__repr__',\n",
       " '__hash__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['visit_id', 'patient_id', 'conditions', 'procedures', 'drugs', 'label', 'gender', 'ethnicity'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic3sample.samples[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'visit_id': {'type': str, 'dim': 0},\n",
       " 'patient_id': {'type': str, 'dim': 0},\n",
       " 'conditions': {'type': str, 'dim': 3},\n",
       " 'procedures': {'type': str, 'dim': 3},\n",
       " 'drugs': {'type': str, 'dim': 3},\n",
       " 'label': {'type': int, 'dim': 0},\n",
       " 'gender': {'type': str, 'dim': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic3sample.input_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(mimic3base.patients['10'], 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'F',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'M',\n",
       " 'F',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic3sample.get_all_tokens(key='gender', remove_duplicates=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'visit_id': {'type': str, 'dim': 0},\n",
       " 'patient_id': {'type': str, 'dim': 0},\n",
       " 'conditions': {'type': str, 'dim': 3},\n",
       " 'procedures': {'type': str, 'dim': 3},\n",
       " 'drugs': {'type': str, 'dim': 3},\n",
       " 'label': {'type': int, 'dim': 0},\n",
       " 'gender': {'type': str, 'dim': 0},\n",
       " 'ethnicity': {'type': str, 'dim': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic3sample.input_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set binary mode to multiclass\n",
      "set binary sens mode to multiclass\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.models import Transformer\n",
    "from models import Adv_Transformer,Laftr_Transformer, LNL_Transformer, CFair_Transformer, LNL_Trainer, AFAL_Transformer, AFAL_Trainer\n",
    "model = AFAL_Transformer(\n",
    "    dataset=mimic3sample,# the dataset should provide sens feat\n",
    "    feature_keys=[\"conditions\", \"procedures\", \"drugs\"], # the model should specify the sens feat\n",
    "    label_key=\"label\",\n",
    "    sens_key='gender',\n",
    "    mode=\"binary\",\n",
    "    sens_mode = 'binary',\n",
    "    alpha=0.0\n",
    "    #fair_coeff=0.0,\n",
    "    #model_var='laftr-dp'\n",
    "    # the model should provide sensitive attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_dummy_param',\n",
       "  Parameter containing:\n",
       "  tensor([], requires_grad=True)),\n",
       " ('embeddings.conditions.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2991, -1.1321, -0.5520,  ..., -1.4299,  0.5636, -0.2250],\n",
       "          [ 0.6280, -0.7186, -1.4664,  ...,  0.3803, -0.8594, -1.4790],\n",
       "          ...,\n",
       "          [-0.7312,  0.7167,  1.4183,  ..., -0.9104,  0.7194, -0.4060],\n",
       "          [ 0.8302,  0.0649,  0.3679,  ...,  1.7737, -0.7138, -1.0214],\n",
       "          [-0.6989, -1.6174,  0.6055,  ..., -0.4188, -2.4469,  0.4839]],\n",
       "         requires_grad=True)),\n",
       " ('embeddings.procedures.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.2388, -0.1898,  2.4136,  ...,  0.7203,  1.9647,  1.1141],\n",
       "          [ 0.1445,  0.4033,  0.7733,  ...,  0.2681, -0.0702,  0.5828],\n",
       "          ...,\n",
       "          [-0.6485,  1.0995,  0.4942,  ..., -1.1189, -0.9046,  0.3469],\n",
       "          [-1.2243, -0.3602,  0.3291,  ..., -0.7500,  1.6559,  0.6894],\n",
       "          [ 1.9451, -0.1433,  0.5630,  ...,  3.1084, -0.4569, -1.0247]],\n",
       "         requires_grad=True)),\n",
       " ('embeddings.drugs.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.2992,  0.5724,  2.0480,  ...,  1.4326,  1.2453,  0.0383],\n",
       "          [ 0.1298, -1.2120,  0.3843,  ..., -0.2982,  1.1401,  1.1843],\n",
       "          ...,\n",
       "          [ 0.1469,  0.8868, -1.0247,  ..., -0.2478, -1.2383, -0.1039],\n",
       "          [-0.4526, -1.7803, -0.1436,  ..., -0.3949,  0.4172, -0.7188],\n",
       "          [ 0.3248,  2.4474,  0.8467,  ...,  1.3051, -1.3558, -0.0031]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.attention.linear_layers.0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0675, -0.0553,  0.0643,  ...,  0.0221, -0.0272, -0.0872],\n",
       "          [-0.0225,  0.0272,  0.0525,  ...,  0.0259,  0.0609, -0.0411],\n",
       "          [ 0.0437, -0.0439, -0.0210,  ...,  0.0160, -0.0260,  0.0414],\n",
       "          ...,\n",
       "          [-0.0268, -0.0760, -0.0665,  ...,  0.0207,  0.0826,  0.0850],\n",
       "          [-0.0052, -0.0448, -0.0281,  ..., -0.0101,  0.0072, -0.0688],\n",
       "          [-0.0046,  0.0570, -0.0390,  ..., -0.0041, -0.0336,  0.0836]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.attention.linear_layers.1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0316, -0.0032,  0.0792,  ..., -0.0672,  0.0779,  0.0290],\n",
       "          [-0.0191,  0.0205, -0.0299,  ...,  0.0439, -0.0627,  0.0313],\n",
       "          [ 0.0328, -0.0880,  0.0769,  ...,  0.0570,  0.0875,  0.0478],\n",
       "          ...,\n",
       "          [-0.0649, -0.0584,  0.0430,  ..., -0.0202, -0.0608,  0.0763],\n",
       "          [-0.0642, -0.0246, -0.0531,  ...,  0.0541,  0.0770,  0.0819],\n",
       "          [ 0.0136, -0.0690, -0.0464,  ..., -0.0318, -0.0566,  0.0577]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.attention.linear_layers.2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0762,  0.0615,  0.0324,  ...,  0.0552,  0.0059,  0.0853],\n",
       "          [ 0.0360,  0.0852,  0.0718,  ...,  0.0187,  0.0663, -0.0551],\n",
       "          [-0.0773,  0.0547,  0.0384,  ...,  0.0360, -0.0436,  0.0067],\n",
       "          ...,\n",
       "          [-0.0496, -0.0218,  0.0810,  ..., -0.0181, -0.0407, -0.0192],\n",
       "          [ 0.0469,  0.0357, -0.0410,  ..., -0.0678, -0.0044, -0.0658],\n",
       "          [ 0.0502,  0.0190, -0.0466,  ...,  0.0790, -0.0670,  0.0130]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.attention.output_linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0731, -0.0678, -0.0238,  ..., -0.0026, -0.0190,  0.0268],\n",
       "          [-0.0095,  0.0243, -0.0570,  ...,  0.0472,  0.0452,  0.0121],\n",
       "          [-0.0832, -0.0850,  0.0503,  ...,  0.0185,  0.0386, -0.0650],\n",
       "          ...,\n",
       "          [-0.0743,  0.0783,  0.0307,  ..., -0.0282, -0.0710,  0.0199],\n",
       "          [-0.0252,  0.0345, -0.0612,  ..., -0.0612,  0.0699,  0.0496],\n",
       "          [ 0.0246,  0.0337,  0.0419,  ..., -0.0794,  0.0778, -0.0042]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.feed_forward.w_1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0385,  0.0013,  0.0473,  ...,  0.0504,  0.0050, -0.0813],\n",
       "          [-0.0860,  0.0865,  0.0066,  ...,  0.0794, -0.0631, -0.0876],\n",
       "          [-0.0684,  0.0277, -0.0619,  ..., -0.0788,  0.0176, -0.0197],\n",
       "          ...,\n",
       "          [ 0.0744, -0.0437, -0.0556,  ...,  0.0488,  0.0221,  0.0634],\n",
       "          [-0.0147, -0.0083,  0.0260,  ..., -0.0365,  0.0011,  0.0182],\n",
       "          [ 0.0455,  0.0385, -0.0140,  ...,  0.0027,  0.0276, -0.0070]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.feed_forward.w_1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0633, -0.0468,  0.0338,  0.0544,  0.0608, -0.0152,  0.0081,  0.0061,\n",
       "          -0.0478,  0.0625,  0.0594, -0.0310, -0.0174,  0.0042,  0.0039, -0.0811,\n",
       "           0.0355, -0.0651,  0.0263,  0.0267,  0.0451, -0.0008,  0.0477, -0.0049,\n",
       "           0.0140, -0.0717, -0.0532,  0.0734,  0.0561, -0.0519, -0.0513, -0.0254,\n",
       "           0.0279,  0.0249,  0.0369,  0.0265,  0.0146,  0.0518,  0.0457, -0.0543,\n",
       "           0.0412, -0.0510, -0.0184,  0.0366, -0.0735, -0.0709,  0.0451,  0.0426,\n",
       "           0.0736,  0.0833, -0.0703,  0.0479,  0.0237,  0.0067,  0.0177,  0.0871,\n",
       "          -0.0501, -0.0689, -0.0715, -0.0381, -0.0211, -0.0234,  0.0299,  0.0493,\n",
       "          -0.0764,  0.0054, -0.0066, -0.0540, -0.0742, -0.0350, -0.0650,  0.0383,\n",
       "          -0.0792, -0.0239,  0.0440, -0.0575, -0.0514,  0.0228, -0.0201, -0.0031,\n",
       "          -0.0050,  0.0016, -0.0145, -0.0086,  0.0193, -0.0407, -0.0165,  0.0167,\n",
       "           0.0441, -0.0310,  0.0774,  0.0459,  0.0867, -0.0821, -0.0177,  0.0167,\n",
       "           0.0795, -0.0325, -0.0668, -0.0485, -0.0104,  0.0502,  0.0183,  0.0010,\n",
       "           0.0081,  0.0078, -0.0701,  0.0548, -0.0697, -0.0041,  0.0406, -0.0028,\n",
       "           0.0790,  0.0248,  0.0815, -0.0664,  0.0677, -0.0600, -0.0138, -0.0460,\n",
       "          -0.0364,  0.0862,  0.0581,  0.0754, -0.0286,  0.0807, -0.0505,  0.0494,\n",
       "           0.0789, -0.0283,  0.0309,  0.0331,  0.0425, -0.0398,  0.0371,  0.0225,\n",
       "          -0.0529, -0.0534, -0.0749,  0.0614,  0.0440, -0.0803,  0.0018,  0.0571,\n",
       "          -0.0708,  0.0118, -0.0148, -0.0882,  0.0531, -0.0261, -0.0760,  0.0629,\n",
       "          -0.0120, -0.0423,  0.0824,  0.0607, -0.0243,  0.0308,  0.0087, -0.0636,\n",
       "           0.0880,  0.0553,  0.0331,  0.0729, -0.0771,  0.0440, -0.0093,  0.0577,\n",
       "           0.0159, -0.0492,  0.0549,  0.0528,  0.0743,  0.0547, -0.0302, -0.0542,\n",
       "           0.0669, -0.0580,  0.0783,  0.0061,  0.0286, -0.0445,  0.0301, -0.0267,\n",
       "          -0.0751, -0.0700,  0.0234, -0.0599,  0.0561,  0.0779, -0.0867, -0.0864,\n",
       "          -0.0094,  0.0646,  0.0451,  0.0638, -0.0211,  0.0161, -0.0349, -0.0642,\n",
       "           0.0405,  0.0695, -0.0822, -0.0278, -0.0548, -0.0834,  0.0793, -0.0863,\n",
       "           0.0661,  0.0160,  0.0528, -0.0653,  0.0392, -0.0009,  0.0465,  0.0218,\n",
       "           0.0591, -0.0749,  0.0326,  0.0671, -0.0333,  0.0874, -0.0369, -0.0603,\n",
       "           0.0649, -0.0813,  0.0328, -0.0188,  0.0450, -0.0409,  0.0680, -0.0872,\n",
       "           0.0609,  0.0396, -0.0793,  0.0493, -0.0247,  0.0369, -0.0029,  0.0651,\n",
       "          -0.0771, -0.0681,  0.0566, -0.0630,  0.0650,  0.0811,  0.0823, -0.0604,\n",
       "          -0.0488, -0.0577, -0.0534,  0.0260,  0.0715, -0.0604,  0.0734, -0.0572,\n",
       "          -0.0493,  0.0579,  0.0772, -0.0535,  0.0215,  0.0330, -0.0798, -0.0518,\n",
       "           0.0030,  0.0188,  0.0446, -0.0197, -0.0367, -0.0689, -0.0662, -0.0194,\n",
       "          -0.0136, -0.0847,  0.0338, -0.0002,  0.0277,  0.0191, -0.0732, -0.0479,\n",
       "          -0.0272, -0.0782, -0.0360,  0.0828, -0.0814,  0.0625, -0.0134, -0.0784,\n",
       "          -0.0861,  0.0269, -0.0573,  0.0320,  0.0417,  0.0042, -0.0553,  0.0129,\n",
       "          -0.0309, -0.0540,  0.0375,  0.0157, -0.0392, -0.0258, -0.0315, -0.0294,\n",
       "          -0.0194, -0.0213, -0.0035,  0.0771,  0.0278, -0.0171, -0.0648,  0.0737,\n",
       "           0.0111, -0.0135, -0.0492,  0.0568,  0.0153, -0.0417, -0.0498, -0.0605,\n",
       "           0.0023,  0.0508, -0.0047, -0.0390,  0.0525,  0.0492,  0.0030, -0.0276,\n",
       "           0.0004, -0.0805,  0.0432, -0.0508,  0.0621, -0.0250, -0.0349,  0.0252,\n",
       "          -0.0259,  0.0428,  0.0002,  0.0123,  0.0765, -0.0592,  0.0180, -0.0607,\n",
       "          -0.0470, -0.0535, -0.0444, -0.0242,  0.0580, -0.0124, -0.0371, -0.0125,\n",
       "           0.0700, -0.0725, -0.0627,  0.0584,  0.0466,  0.0548,  0.0654, -0.0330,\n",
       "          -0.0407, -0.0422, -0.0027, -0.0307, -0.0601, -0.0492,  0.0138,  0.0882,\n",
       "           0.0253,  0.0203, -0.0168, -0.0676, -0.0375, -0.0859, -0.0880, -0.0324,\n",
       "           0.0213, -0.0024, -0.0634,  0.0359,  0.0404, -0.0449, -0.0240,  0.0572,\n",
       "          -0.0045,  0.0187,  0.0420,  0.0716, -0.0636,  0.0341, -0.0286,  0.0373,\n",
       "           0.0114, -0.0848,  0.0435,  0.0858, -0.0343,  0.0656, -0.0057,  0.0850,\n",
       "          -0.0818, -0.0249, -0.0318, -0.0784,  0.0805,  0.0431,  0.0523,  0.0728,\n",
       "          -0.0189,  0.0028,  0.0143,  0.0182,  0.0128, -0.0568, -0.0101,  0.0324,\n",
       "          -0.0415,  0.0807,  0.0371,  0.0272,  0.0603,  0.0184, -0.0398,  0.0500,\n",
       "           0.0400, -0.0141,  0.0881, -0.0185,  0.0450, -0.0324, -0.0836, -0.0751,\n",
       "          -0.0352,  0.0585,  0.0670, -0.0624, -0.0834, -0.0864,  0.0230, -0.0180,\n",
       "           0.0007, -0.0237, -0.0559, -0.0734, -0.0227, -0.0457, -0.0520, -0.0517,\n",
       "          -0.0303,  0.0687, -0.0593, -0.0022, -0.0702, -0.0760,  0.0740,  0.0158,\n",
       "           0.0030,  0.0129,  0.0442, -0.0340,  0.0170, -0.0564,  0.0107,  0.0718,\n",
       "          -0.0787,  0.0663,  0.0155, -0.0358,  0.0150,  0.0262, -0.0690, -0.0704,\n",
       "          -0.0516, -0.0866,  0.0141, -0.0186, -0.0269,  0.0451,  0.0793, -0.0749,\n",
       "          -0.0680,  0.0551,  0.0201,  0.0490,  0.0528,  0.0784,  0.0049,  0.0598,\n",
       "           0.0299,  0.0242,  0.0446,  0.0611,  0.0564, -0.0441,  0.0157, -0.0159,\n",
       "          -0.0411,  0.0854,  0.0831,  0.0604,  0.0105,  0.0802,  0.0340,  0.0446,\n",
       "           0.0677,  0.0303, -0.0176, -0.0704,  0.0102,  0.0710, -0.0884,  0.0280],\n",
       "         requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.feed_forward.w_2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0179,  0.0236,  0.0330,  ...,  0.0223,  0.0230,  0.0352],\n",
       "          [-0.0199,  0.0369,  0.0379,  ...,  0.0019, -0.0289,  0.0270],\n",
       "          [-0.0222, -0.0139, -0.0131,  ..., -0.0051,  0.0003,  0.0160],\n",
       "          ...,\n",
       "          [ 0.0088, -0.0211,  0.0044,  ..., -0.0132, -0.0222, -0.0021],\n",
       "          [ 0.0319,  0.0436,  0.0217,  ...,  0.0326, -0.0150, -0.0402],\n",
       "          [ 0.0120,  0.0033,  0.0266,  ...,  0.0269,  0.0428,  0.0437]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.feed_forward.w_2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-1.1588e-02, -1.3681e-02,  2.7497e-02,  7.4241e-03,  1.4541e-02,\n",
       "          -1.2582e-02, -1.4170e-02, -1.5787e-02,  1.7549e-02,  1.2309e-02,\n",
       "           8.1575e-03, -2.8252e-03, -9.6047e-03, -3.6222e-02, -3.8656e-02,\n",
       "          -2.4161e-05,  1.7802e-03, -2.4524e-02, -1.1628e-02, -4.5635e-03,\n",
       "           2.0958e-02, -1.8365e-02,  2.4766e-02,  1.3696e-02, -3.7338e-02,\n",
       "           5.7445e-03, -3.2719e-03, -5.2522e-03, -4.3629e-02,  3.4580e-02,\n",
       "          -3.1168e-02,  2.6832e-03, -2.2443e-02, -3.0739e-02,  2.2022e-02,\n",
       "           3.6887e-03,  4.1358e-02, -3.0696e-03,  2.7392e-02,  2.3512e-02,\n",
       "          -4.0514e-02,  9.5911e-03,  3.2052e-02,  1.8272e-02, -3.4657e-02,\n",
       "           2.7290e-02, -3.0913e-02,  4.3923e-02, -2.9086e-02,  3.0759e-02,\n",
       "           3.3774e-02, -6.4551e-03, -2.7423e-02,  7.7490e-03,  3.5477e-02,\n",
       "          -1.9129e-02, -4.0143e-02, -2.7720e-02,  9.3352e-04,  4.2506e-02,\n",
       "           3.1285e-02, -2.6682e-03, -2.5619e-02, -1.9527e-02, -4.2023e-02,\n",
       "          -7.3116e-04,  2.7580e-02, -2.9477e-02, -4.8736e-03, -2.0827e-02,\n",
       "          -3.4502e-02,  1.4684e-02,  3.3538e-02, -3.1014e-02, -2.2874e-03,\n",
       "           1.9088e-02, -3.9196e-02, -5.8533e-03, -1.7290e-02, -5.9388e-03,\n",
       "          -6.9399e-03, -2.5281e-02,  3.2201e-02,  1.3421e-02,  9.4534e-03,\n",
       "           3.0581e-02, -1.7444e-02, -3.9926e-02,  4.0903e-02, -2.0821e-02,\n",
       "           4.4777e-03,  7.7999e-03,  4.3975e-02,  1.7052e-02, -9.4485e-03,\n",
       "           3.4225e-02, -3.6176e-02, -1.1736e-02,  1.9068e-02, -4.4629e-03,\n",
       "           1.9975e-02,  2.0679e-02,  1.2219e-02, -3.5451e-02,  1.8152e-02,\n",
       "          -4.1281e-02,  1.5114e-02,  4.2113e-02, -4.4355e-03,  1.7533e-02,\n",
       "           5.3922e-03,  2.1500e-02,  3.0304e-02,  3.2108e-02, -3.3042e-02,\n",
       "           4.2940e-02, -4.3256e-02, -3.6041e-02, -4.3160e-02, -1.6191e-02,\n",
       "           4.3991e-02, -4.2115e-02, -2.3687e-02,  1.8147e-02,  1.6622e-02,\n",
       "           4.2980e-02,  6.4938e-03,  9.3648e-03], requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.input_sublayer.norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.], requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.input_sublayer.norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.output_sublayer.norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.], requires_grad=True)),\n",
       " ('transformer.conditions.transformer.0.output_sublayer.norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.attention.linear_layers.0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0134, -0.0555, -0.0816,  ..., -0.0608, -0.0423, -0.0772],\n",
       "          [ 0.0461, -0.0089,  0.0518,  ..., -0.0423,  0.0564,  0.0454],\n",
       "          [ 0.0402,  0.0800,  0.0412,  ...,  0.0790,  0.0232,  0.0151],\n",
       "          ...,\n",
       "          [ 0.0656,  0.0138, -0.0854,  ...,  0.0530, -0.0860, -0.0636],\n",
       "          [-0.0009, -0.0722, -0.0471,  ..., -0.0479, -0.0429,  0.0737],\n",
       "          [-0.0640, -0.0080,  0.0229,  ...,  0.0664, -0.0726, -0.0188]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.attention.linear_layers.1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0278,  0.0655,  0.0116,  ..., -0.0010,  0.0539, -0.0462],\n",
       "          [ 0.0045, -0.0212,  0.0212,  ...,  0.0055,  0.0883, -0.0695],\n",
       "          [-0.0449,  0.0254, -0.0707,  ...,  0.0822,  0.0289,  0.0843],\n",
       "          ...,\n",
       "          [-0.0683, -0.0515, -0.0559,  ..., -0.0152, -0.0286, -0.0256],\n",
       "          [-0.0821, -0.0686, -0.0413,  ...,  0.0399,  0.0550,  0.0299],\n",
       "          [ 0.0231, -0.0707,  0.0496,  ...,  0.0452,  0.0302,  0.0546]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.attention.linear_layers.2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0625,  0.0774, -0.0682,  ..., -0.0696, -0.0026,  0.0468],\n",
       "          [-0.0648,  0.0124, -0.0709,  ..., -0.0256, -0.0549, -0.0629],\n",
       "          [-0.0288, -0.0619, -0.0361,  ..., -0.0686, -0.0585,  0.0162],\n",
       "          ...,\n",
       "          [ 0.0289,  0.0235,  0.0092,  ...,  0.0612, -0.0174, -0.0140],\n",
       "          [-0.0635, -0.0209, -0.0672,  ..., -0.0551, -0.0366, -0.0524],\n",
       "          [-0.0363, -0.0166,  0.0236,  ...,  0.0091, -0.0181, -0.0759]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.attention.output_linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0401,  0.0601,  0.0292,  ..., -0.0526, -0.0719, -0.0660],\n",
       "          [-0.0098, -0.0296, -0.0297,  ...,  0.0857, -0.0332, -0.0282],\n",
       "          [-0.0872,  0.0518,  0.0846,  ...,  0.0506, -0.0322, -0.0172],\n",
       "          ...,\n",
       "          [ 0.0854,  0.0340,  0.0119,  ...,  0.0330, -0.0476, -0.0113],\n",
       "          [-0.0806, -0.0390, -0.0801,  ...,  0.0279, -0.0232,  0.0529],\n",
       "          [ 0.0500, -0.0324,  0.0036,  ..., -0.0207,  0.0433,  0.0293]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.feed_forward.w_1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0220, -0.0336,  0.0726,  ..., -0.0699,  0.0024, -0.0626],\n",
       "          [-0.0027,  0.0129, -0.0470,  ..., -0.0252,  0.0526,  0.0161],\n",
       "          [-0.0309, -0.0713,  0.0767,  ...,  0.0772,  0.0555,  0.0878],\n",
       "          ...,\n",
       "          [ 0.0825, -0.0050, -0.0566,  ..., -0.0588,  0.0472,  0.0374],\n",
       "          [ 0.0849,  0.0754, -0.0158,  ..., -0.0108,  0.0283, -0.0549],\n",
       "          [-0.0412,  0.0438,  0.0322,  ..., -0.0408,  0.0222,  0.0411]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.feed_forward.w_1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 2.8725e-02,  4.6620e-02, -6.1120e-02, -2.2978e-02, -3.1701e-02,\n",
       "           6.2472e-02,  1.5954e-02,  3.1929e-02, -4.7922e-02,  2.6448e-03,\n",
       "          -6.4065e-02, -6.1458e-02, -2.9045e-02, -8.0754e-02,  4.0160e-02,\n",
       "           6.0889e-03, -1.0063e-02,  7.0405e-02,  6.3457e-02,  4.7206e-03,\n",
       "           8.6674e-02,  1.2275e-02,  4.5978e-02, -4.9990e-02,  2.5891e-02,\n",
       "           7.6045e-02, -2.4739e-03,  1.4552e-02,  2.2833e-02, -8.4915e-02,\n",
       "          -6.7755e-02,  7.4293e-02, -1.0861e-02,  3.4072e-02,  4.2671e-02,\n",
       "           6.0828e-02,  6.3760e-02, -6.3641e-02,  4.1007e-02,  2.5053e-02,\n",
       "          -6.7273e-02,  3.6655e-03, -6.4484e-02,  5.0749e-02, -2.2598e-02,\n",
       "           5.1220e-02,  4.1023e-02, -3.4119e-02, -3.3490e-02,  1.5454e-02,\n",
       "           7.9570e-03, -2.0253e-02,  8.2714e-03,  1.6059e-02,  2.6534e-02,\n",
       "          -1.8273e-02,  3.4766e-02, -1.8431e-02,  7.8041e-02, -6.9938e-03,\n",
       "          -7.7403e-02, -1.6402e-02,  1.7597e-02, -7.1668e-02, -1.1288e-02,\n",
       "          -8.6135e-02, -4.4787e-02,  8.7691e-02, -6.7686e-02, -3.1641e-02,\n",
       "          -1.2075e-02,  1.0987e-02,  1.0834e-02, -1.6541e-02, -3.6261e-02,\n",
       "           3.9831e-02, -1.6661e-02, -4.9682e-03, -5.7961e-02, -8.2489e-02,\n",
       "          -2.5377e-02,  5.6727e-02,  5.9745e-02, -5.6263e-03,  1.7813e-02,\n",
       "           8.0961e-03, -1.6965e-02, -8.3686e-02,  6.8036e-02, -1.7295e-02,\n",
       "          -1.4030e-03, -3.1093e-02, -6.8269e-03, -6.0803e-02,  4.9292e-03,\n",
       "          -8.1176e-03,  7.1529e-02,  9.7538e-03,  1.2977e-02, -2.4920e-02,\n",
       "           8.0704e-02, -7.9200e-02,  2.2972e-02, -6.9444e-02, -8.5205e-02,\n",
       "           5.0934e-02,  5.9974e-02, -8.3142e-03,  5.1653e-02,  4.5966e-02,\n",
       "          -1.2788e-02,  8.2883e-03,  1.3875e-02,  7.4130e-02, -1.2810e-02,\n",
       "           7.6731e-02, -4.2188e-02, -2.0210e-02, -5.8403e-02, -3.3081e-02,\n",
       "          -8.2430e-02, -1.0621e-02,  3.0439e-02, -4.4842e-02,  2.3396e-02,\n",
       "          -5.3796e-02, -7.9915e-02, -4.7416e-02, -1.8654e-02,  2.2047e-02,\n",
       "          -6.1314e-02,  3.4210e-02, -4.8710e-02, -1.8667e-02,  7.0624e-02,\n",
       "           3.3398e-02,  3.6120e-02,  3.9135e-03,  4.7674e-02,  4.2010e-02,\n",
       "           6.2119e-02,  6.3841e-03,  8.4564e-02, -4.1671e-02,  1.1907e-02,\n",
       "          -2.4770e-02,  8.4582e-02, -5.9646e-02,  7.3888e-02,  7.9682e-02,\n",
       "          -6.8996e-02,  6.2905e-02, -2.8069e-02, -5.5028e-02, -3.7969e-02,\n",
       "           5.5754e-02, -3.4559e-02,  6.6670e-02, -9.5406e-03,  7.4197e-02,\n",
       "          -2.8503e-02, -4.1151e-02, -5.9596e-02,  2.0146e-02, -4.9668e-02,\n",
       "           5.5096e-02, -8.3766e-02, -8.6525e-02,  4.3761e-02,  8.7739e-05,\n",
       "          -3.5192e-02,  4.4149e-03, -7.0914e-02,  3.4161e-02,  6.9328e-02,\n",
       "          -4.7988e-02, -3.3321e-02, -2.6335e-02, -5.2727e-02,  5.2399e-02,\n",
       "          -6.5248e-02, -1.4964e-02,  7.5391e-02, -1.5216e-02,  1.7345e-03,\n",
       "           7.4634e-02, -4.6365e-02, -6.1711e-02, -5.3693e-02, -4.2135e-03,\n",
       "           8.0102e-03,  1.8475e-02, -2.4434e-02,  1.3008e-02, -5.0514e-02,\n",
       "          -8.4847e-02,  6.7356e-03,  8.1729e-02, -2.0706e-03, -3.2371e-02,\n",
       "           3.5706e-02, -3.6103e-02, -8.4783e-02, -2.6482e-02,  6.3537e-02,\n",
       "           6.1707e-02,  8.1993e-02,  3.8504e-02, -3.4336e-02,  3.4239e-02,\n",
       "          -3.5528e-02,  3.4549e-02, -2.9913e-02,  1.1872e-02,  5.3014e-02,\n",
       "          -3.5301e-02,  5.9523e-02,  6.7882e-02,  7.4112e-02,  1.0062e-02,\n",
       "           7.5480e-02, -7.1809e-02, -1.6506e-02, -4.0450e-02, -4.4580e-02,\n",
       "           7.2015e-02, -5.4452e-03, -3.3603e-02, -4.0870e-02, -8.1712e-02,\n",
       "           1.5552e-04, -3.2062e-02, -2.6562e-03, -3.6914e-02,  5.4178e-02,\n",
       "          -2.2028e-02,  8.2431e-02, -9.1473e-03,  5.8045e-02,  5.7417e-02,\n",
       "          -4.8618e-02,  7.6406e-02,  3.6498e-02, -7.3037e-02,  7.9102e-03,\n",
       "          -7.5260e-03, -7.1953e-02,  6.8718e-02, -2.6870e-02,  3.0837e-02,\n",
       "          -8.0228e-02, -4.7187e-02, -6.9833e-02, -3.7791e-02, -7.6645e-02,\n",
       "           7.2669e-02,  6.5382e-02,  5.9639e-02, -1.5485e-02, -2.0096e-02,\n",
       "           6.7722e-02, -8.0581e-02, -3.5407e-02, -5.6143e-02, -1.0742e-03,\n",
       "           1.5255e-02, -9.9389e-03, -3.6236e-02,  7.5897e-03,  8.4166e-02,\n",
       "           5.5896e-02,  3.3548e-02,  4.8741e-02,  6.7557e-02,  3.7503e-02,\n",
       "          -6.2867e-02,  6.8193e-02,  6.1215e-02, -8.6631e-02, -3.1945e-02,\n",
       "          -5.6074e-02,  7.8864e-03,  2.3807e-02, -6.0617e-02, -6.5358e-02,\n",
       "          -7.8370e-03,  4.7312e-02, -7.8572e-02, -3.3444e-02, -3.1000e-02,\n",
       "           7.8692e-02,  4.6611e-02,  4.6965e-02,  6.7282e-02,  2.0540e-02,\n",
       "           1.3140e-02,  6.2117e-02,  3.8666e-02, -4.8696e-02,  8.8384e-02,\n",
       "          -5.9347e-02,  1.2143e-02, -8.1998e-02, -8.1155e-02, -5.5198e-02,\n",
       "          -7.4642e-02, -4.5648e-02,  4.6155e-02,  5.4565e-02, -1.5208e-02,\n",
       "           1.7616e-02, -4.1369e-02, -4.5306e-02,  4.7530e-02, -3.5810e-03,\n",
       "          -1.5916e-02,  3.3629e-02, -8.3738e-02, -2.8034e-02, -4.9944e-02,\n",
       "          -5.5846e-02, -6.6698e-02,  7.2502e-03,  5.3004e-02, -4.1820e-02,\n",
       "          -8.2803e-02,  2.2452e-02,  2.1486e-02, -7.8809e-02,  2.6769e-02,\n",
       "          -1.0020e-02, -3.5995e-02, -4.8104e-02, -6.5050e-02, -5.0950e-02,\n",
       "           6.3991e-02, -2.6467e-02,  1.6954e-02, -2.7326e-02,  5.9208e-02,\n",
       "          -8.6469e-02, -8.6069e-02, -6.2753e-02,  8.5439e-02,  6.1061e-04,\n",
       "          -4.8468e-02,  3.5894e-02, -2.7595e-02,  1.6542e-02, -2.8439e-02,\n",
       "           2.6003e-02,  4.6291e-02, -1.8167e-02,  1.5130e-02,  5.0433e-02,\n",
       "          -7.0143e-02,  1.0793e-03, -5.4663e-02,  1.0089e-02, -5.3889e-02,\n",
       "          -6.2313e-02, -3.9344e-02, -8.4959e-02, -3.6397e-02,  6.0888e-02,\n",
       "          -2.3868e-02,  2.8568e-02,  4.2290e-02, -1.1134e-02,  4.3473e-02,\n",
       "           7.7915e-02, -3.9031e-02,  8.1895e-02, -3.0531e-02, -7.4667e-02,\n",
       "          -6.7369e-02, -3.9997e-02,  6.3374e-02,  1.1025e-02, -5.2282e-02,\n",
       "          -8.2785e-02, -1.1341e-03, -6.3423e-03,  7.7309e-02,  8.0390e-02,\n",
       "           4.2244e-02,  7.1389e-02, -7.8823e-02, -3.2118e-02,  2.7440e-02,\n",
       "          -1.8515e-02, -3.0977e-02, -6.4295e-02,  4.0922e-02, -1.3889e-02,\n",
       "           6.2322e-02, -1.0291e-03, -6.0973e-02,  2.7107e-03,  5.4296e-02,\n",
       "          -3.7021e-02, -7.8562e-02, -8.6909e-02,  7.8262e-03,  6.2844e-02,\n",
       "           6.0294e-02,  5.4818e-02,  1.3710e-02,  7.4461e-02,  1.9950e-03,\n",
       "          -4.6417e-02,  8.3334e-02,  5.0252e-03, -5.2016e-02, -3.6964e-02,\n",
       "          -8.8029e-02, -1.3499e-02,  2.1882e-02,  3.9138e-02,  2.3011e-02,\n",
       "          -2.9616e-02, -2.9290e-02,  6.6212e-03,  4.1336e-02, -4.1714e-02,\n",
       "          -2.9343e-02,  6.1402e-03,  7.1481e-02, -8.8060e-02,  4.9054e-02,\n",
       "          -2.7360e-02,  7.6616e-02,  3.6671e-02, -1.8201e-02,  8.1063e-02,\n",
       "          -7.9005e-02,  7.3347e-02,  3.3110e-02,  6.7171e-02, -3.1868e-02,\n",
       "           5.7705e-03, -3.3099e-02, -8.0665e-02,  2.6599e-02,  7.7130e-02,\n",
       "          -3.4138e-02, -3.5225e-02,  2.4026e-02, -8.7593e-03,  4.9654e-02,\n",
       "          -3.1282e-02,  6.6946e-04,  3.3834e-02, -7.1489e-02,  1.3418e-02,\n",
       "          -4.7368e-02,  4.6823e-03,  7.9845e-02, -9.1355e-03,  1.6143e-03,\n",
       "          -8.6445e-02, -8.4037e-02, -5.8469e-02,  2.8910e-02, -2.9392e-02,\n",
       "          -5.4462e-02,  4.8444e-02, -6.6298e-02,  3.8633e-03, -1.9926e-02,\n",
       "          -4.9886e-03, -8.6371e-02,  2.2011e-03,  2.2800e-02,  6.9724e-02,\n",
       "          -6.4121e-04, -7.7395e-03,  6.8964e-02, -5.8953e-02, -1.7200e-02,\n",
       "           3.6178e-02, -1.5682e-02, -5.2483e-02, -3.4721e-02, -8.2076e-02,\n",
       "           3.7645e-02, -7.5301e-02, -7.0082e-02, -3.1804e-02, -1.1571e-02,\n",
       "           9.9446e-03, -6.0049e-02, -2.4045e-02, -5.4048e-02, -4.3327e-02,\n",
       "          -6.1806e-02, -2.3206e-02,  6.0398e-02, -7.7493e-02, -3.6761e-02,\n",
       "          -3.2197e-02, -5.6290e-02,  8.1113e-03,  6.7851e-03, -3.9576e-02,\n",
       "          -6.4596e-02, -7.6418e-03, -7.6084e-02, -5.0212e-02, -6.0147e-02,\n",
       "           5.1032e-02,  5.3957e-02], requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.feed_forward.w_2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0151, -0.0189,  0.0087,  ..., -0.0016,  0.0031, -0.0315],\n",
       "          [ 0.0310, -0.0394,  0.0341,  ..., -0.0037, -0.0061,  0.0135],\n",
       "          [ 0.0069,  0.0319, -0.0300,  ...,  0.0186, -0.0088,  0.0329],\n",
       "          ...,\n",
       "          [ 0.0273,  0.0045,  0.0090,  ...,  0.0315, -0.0345, -0.0169],\n",
       "          [-0.0416, -0.0242,  0.0339,  ...,  0.0272,  0.0143, -0.0092],\n",
       "          [ 0.0156,  0.0234, -0.0030,  ...,  0.0359,  0.0180,  0.0384]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.feed_forward.w_2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0330,  0.0421,  0.0203, -0.0035, -0.0251,  0.0136,  0.0063,  0.0031,\n",
       "           0.0202, -0.0439, -0.0262, -0.0145, -0.0290,  0.0170,  0.0420,  0.0010,\n",
       "          -0.0091,  0.0403, -0.0242,  0.0246, -0.0083,  0.0363, -0.0198,  0.0179,\n",
       "          -0.0301,  0.0393,  0.0434,  0.0381, -0.0150, -0.0044, -0.0397,  0.0131,\n",
       "           0.0132, -0.0194,  0.0127,  0.0390, -0.0023, -0.0120, -0.0364,  0.0283,\n",
       "          -0.0079,  0.0005,  0.0256, -0.0232,  0.0050, -0.0060, -0.0393,  0.0358,\n",
       "          -0.0319,  0.0386,  0.0387, -0.0121, -0.0146,  0.0208,  0.0076, -0.0053,\n",
       "          -0.0250,  0.0219, -0.0030, -0.0441,  0.0364, -0.0323,  0.0305,  0.0063,\n",
       "          -0.0422, -0.0009,  0.0129,  0.0293,  0.0047, -0.0373,  0.0364, -0.0136,\n",
       "           0.0169, -0.0013, -0.0213,  0.0375, -0.0032,  0.0188, -0.0315, -0.0053,\n",
       "           0.0032,  0.0412, -0.0162,  0.0218, -0.0272,  0.0273,  0.0022, -0.0402,\n",
       "           0.0013,  0.0322,  0.0175,  0.0146,  0.0066,  0.0293, -0.0051, -0.0023,\n",
       "          -0.0415,  0.0201, -0.0008,  0.0253, -0.0424,  0.0423, -0.0270,  0.0412,\n",
       "          -0.0207, -0.0159, -0.0322,  0.0041, -0.0165,  0.0146, -0.0154,  0.0385,\n",
       "           0.0297, -0.0210,  0.0282, -0.0332, -0.0401,  0.0213,  0.0191,  0.0035,\n",
       "          -0.0177, -0.0152, -0.0101,  0.0062,  0.0182, -0.0019,  0.0202,  0.0393],\n",
       "         requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.input_sublayer.norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.], requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.input_sublayer.norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.output_sublayer.norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.], requires_grad=True)),\n",
       " ('transformer.procedures.transformer.0.output_sublayer.norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.attention.linear_layers.0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0685,  0.0706,  0.0155,  ..., -0.0441, -0.0520,  0.0103],\n",
       "          [ 0.0098,  0.0553, -0.0689,  ..., -0.0647,  0.0041, -0.0171],\n",
       "          [-0.0423,  0.0075,  0.0185,  ...,  0.0503,  0.0350,  0.0413],\n",
       "          ...,\n",
       "          [ 0.0818, -0.0356, -0.0495,  ...,  0.0215,  0.0289, -0.0673],\n",
       "          [ 0.0803, -0.0444,  0.0098,  ...,  0.0587,  0.0480,  0.0048],\n",
       "          [ 0.0033,  0.0213, -0.0217,  ..., -0.0354, -0.0500, -0.0322]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.attention.linear_layers.1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0218,  0.0728, -0.0142,  ...,  0.0233, -0.0818, -0.0664],\n",
       "          [ 0.0336,  0.0524,  0.0250,  ...,  0.0237, -0.0874,  0.0230],\n",
       "          [-0.0542, -0.0532, -0.0178,  ..., -0.0794,  0.0693,  0.0260],\n",
       "          ...,\n",
       "          [ 0.0821,  0.0408,  0.0639,  ..., -0.0338,  0.0496,  0.0850],\n",
       "          [-0.0527,  0.0283,  0.0823,  ..., -0.0140, -0.0054, -0.0594],\n",
       "          [-0.0486,  0.0827, -0.0731,  ..., -0.0547, -0.0629, -0.0839]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.attention.linear_layers.2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0538, -0.0693, -0.0019,  ..., -0.0328,  0.0049, -0.0783],\n",
       "          [-0.0445, -0.0008,  0.0507,  ..., -0.0598,  0.0057,  0.0817],\n",
       "          [ 0.0066, -0.0651, -0.0084,  ...,  0.0271,  0.0837, -0.0317],\n",
       "          ...,\n",
       "          [ 0.0303,  0.0178, -0.0685,  ...,  0.0755,  0.0215, -0.0681],\n",
       "          [-0.0003,  0.0340, -0.0213,  ..., -0.0098, -0.0035, -0.0317],\n",
       "          [ 0.0751,  0.0167, -0.0094,  ..., -0.0560, -0.0746, -0.0784]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.attention.output_linear.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0152, -0.0832, -0.0346,  ...,  0.0091, -0.0735,  0.0731],\n",
       "          [-0.0516,  0.0352,  0.0464,  ..., -0.0718,  0.0261, -0.0382],\n",
       "          [ 0.0290, -0.0769,  0.0446,  ..., -0.0040,  0.0009, -0.0116],\n",
       "          ...,\n",
       "          [-0.0069, -0.0482, -0.0091,  ...,  0.0578,  0.0365,  0.0664],\n",
       "          [-0.0283,  0.0513, -0.0876,  ..., -0.0203, -0.0561, -0.0794],\n",
       "          [-0.0723,  0.0365,  0.0131,  ...,  0.0258,  0.0539, -0.0519]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.feed_forward.w_1.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0662, -0.0746,  0.0443,  ..., -0.0083,  0.0800,  0.0410],\n",
       "          [ 0.0508, -0.0138, -0.0366,  ...,  0.0859,  0.0782,  0.0704],\n",
       "          [ 0.0126,  0.0245, -0.0682,  ...,  0.0693,  0.0700, -0.0804],\n",
       "          ...,\n",
       "          [ 0.0542, -0.0834, -0.0324,  ..., -0.0407,  0.0004, -0.0223],\n",
       "          [-0.0432, -0.0380, -0.0070,  ..., -0.0594, -0.0833, -0.0294],\n",
       "          [ 0.0714,  0.0084,  0.0233,  ...,  0.0145, -0.0713,  0.0731]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.feed_forward.w_1.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0628,  0.0762,  0.0310, -0.0480, -0.0881,  0.0024, -0.0156,  0.0583,\n",
       "          -0.0631, -0.0205,  0.0134,  0.0386, -0.0554,  0.0866,  0.0824,  0.0864,\n",
       "          -0.0422,  0.0338, -0.0049, -0.0183, -0.0069,  0.0873,  0.0649,  0.0680,\n",
       "          -0.0629,  0.0030,  0.0294, -0.0719,  0.0293, -0.0493,  0.0752, -0.0539,\n",
       "           0.0061,  0.0394, -0.0718,  0.0458,  0.0264, -0.0656, -0.0464,  0.0238,\n",
       "           0.0497, -0.0100, -0.0393,  0.0142, -0.0794,  0.0669,  0.0399,  0.0273,\n",
       "           0.0596, -0.0340,  0.0146,  0.0061, -0.0246, -0.0348,  0.0077,  0.0742,\n",
       "          -0.0016, -0.0256,  0.0431,  0.0343,  0.0601,  0.0302,  0.0436, -0.0460,\n",
       "           0.0189,  0.0105, -0.0143,  0.0860,  0.0613,  0.0478, -0.0409, -0.0071,\n",
       "          -0.0511, -0.0588, -0.0041, -0.0683,  0.0297,  0.0369, -0.0535,  0.0089,\n",
       "           0.0119, -0.0303, -0.0181, -0.0271, -0.0272,  0.0183, -0.0751,  0.0571,\n",
       "          -0.0452,  0.0150, -0.0117,  0.0053, -0.0121, -0.0155, -0.0229,  0.0354,\n",
       "          -0.0808, -0.0245, -0.0375, -0.0524,  0.0180, -0.0596,  0.0281, -0.0883,\n",
       "          -0.0729,  0.0499, -0.0317,  0.0443,  0.0543, -0.0819, -0.0447, -0.0746,\n",
       "           0.0646,  0.0073,  0.0643, -0.0125, -0.0654, -0.0443,  0.0624,  0.0182,\n",
       "          -0.0630,  0.0747, -0.0628,  0.0139,  0.0091, -0.0500, -0.0362,  0.0634,\n",
       "           0.0528, -0.0511, -0.0565,  0.0097,  0.0453, -0.0013, -0.0634, -0.0045,\n",
       "           0.0761, -0.0543, -0.0010,  0.0830, -0.0738, -0.0516, -0.0489,  0.0208,\n",
       "           0.0512,  0.0786,  0.0549,  0.0744,  0.0201,  0.0645, -0.0378,  0.0068,\n",
       "           0.0805, -0.0350,  0.0204,  0.0828, -0.0603,  0.0447, -0.0664,  0.0616,\n",
       "          -0.0111, -0.0634, -0.0386, -0.0753,  0.0588, -0.0307,  0.0336,  0.0850,\n",
       "           0.0840,  0.0307,  0.0745,  0.0837,  0.0143,  0.0255, -0.0506, -0.0181,\n",
       "          -0.0165, -0.0440,  0.0427, -0.0421,  0.0287, -0.0168,  0.0065, -0.0772,\n",
       "           0.0209,  0.0297,  0.0280,  0.0751,  0.0194, -0.0183,  0.0752,  0.0346,\n",
       "          -0.0545,  0.0717,  0.0489, -0.0498,  0.0677, -0.0693, -0.0841,  0.0041,\n",
       "          -0.0548, -0.0563, -0.0716,  0.0847, -0.0865, -0.0271, -0.0880, -0.0055,\n",
       "          -0.0074, -0.0098,  0.0461, -0.0822, -0.0082,  0.0252, -0.0403,  0.0239,\n",
       "           0.0163,  0.0533, -0.0300, -0.0573, -0.0859, -0.0531, -0.0539, -0.0039,\n",
       "          -0.0585, -0.0327,  0.0545,  0.0108, -0.0589, -0.0630, -0.0839,  0.0849,\n",
       "           0.0725,  0.0386,  0.0850, -0.0523,  0.0303, -0.0793,  0.0291,  0.0732,\n",
       "          -0.0054, -0.0839, -0.0550,  0.0641, -0.0598, -0.0244, -0.0214, -0.0323,\n",
       "           0.0462,  0.0131, -0.0125,  0.0503,  0.0418, -0.0301, -0.0738,  0.0249,\n",
       "          -0.0872,  0.0871,  0.0728, -0.0820,  0.0788, -0.0844,  0.0492,  0.0743,\n",
       "          -0.0525,  0.0375, -0.0514, -0.0496,  0.0543, -0.0107,  0.0549,  0.0445,\n",
       "           0.0302, -0.0699, -0.0533, -0.0055,  0.0395,  0.0304, -0.0705, -0.0442,\n",
       "          -0.0671,  0.0359, -0.0839, -0.0388, -0.0257, -0.0512, -0.0404, -0.0301,\n",
       "           0.0647, -0.0141,  0.0486,  0.0080,  0.0300,  0.0308, -0.0533, -0.0758,\n",
       "          -0.0751, -0.0385,  0.0320,  0.0698,  0.0552,  0.0703,  0.0002,  0.0450,\n",
       "          -0.0029,  0.0175,  0.0703,  0.0235,  0.0819,  0.0139,  0.0831,  0.0261,\n",
       "           0.0167,  0.0296, -0.0574,  0.0867,  0.0249, -0.0279,  0.0414, -0.0677,\n",
       "           0.0740,  0.0763,  0.0717, -0.0459,  0.0178, -0.0221,  0.0699, -0.0581,\n",
       "           0.0687,  0.0489,  0.0345,  0.0718,  0.0777, -0.0622,  0.0067, -0.0211,\n",
       "           0.0165, -0.0132,  0.0856, -0.0548,  0.0252,  0.0472, -0.0288,  0.0149,\n",
       "           0.0543, -0.0745,  0.0135, -0.0249,  0.0547,  0.0613, -0.0226,  0.0458,\n",
       "          -0.0841,  0.0266,  0.0287, -0.0714,  0.0664, -0.0371, -0.0356, -0.0228,\n",
       "          -0.0721, -0.0307,  0.0371, -0.0512,  0.0239,  0.0362,  0.0092,  0.0239,\n",
       "          -0.0230, -0.0113,  0.0271,  0.0379,  0.0225,  0.0144, -0.0186,  0.0486,\n",
       "           0.0565,  0.0132,  0.0147, -0.0604, -0.0554, -0.0799, -0.0809,  0.0840,\n",
       "           0.0284, -0.0550, -0.0472,  0.0726, -0.0469,  0.0388, -0.0869,  0.0446,\n",
       "          -0.0102,  0.0281, -0.0433,  0.0055, -0.0549, -0.0538,  0.0324,  0.0231,\n",
       "          -0.0641,  0.0211,  0.0753, -0.0303, -0.0311,  0.0687, -0.0865,  0.0693,\n",
       "          -0.0193,  0.0452,  0.0691, -0.0133, -0.0377,  0.0565,  0.0203, -0.0058,\n",
       "           0.0781,  0.0774, -0.0510,  0.0462, -0.0162,  0.0161, -0.0777,  0.0776,\n",
       "          -0.0300,  0.0151, -0.0682, -0.0573, -0.0073,  0.0873,  0.0321,  0.0601,\n",
       "           0.0665, -0.0284, -0.0194,  0.0395,  0.0186,  0.0345, -0.0425, -0.0592,\n",
       "          -0.0540, -0.0879,  0.0623, -0.0001, -0.0517, -0.0217, -0.0401,  0.0083,\n",
       "           0.0522, -0.0519,  0.0026,  0.0209,  0.0869,  0.0229, -0.0420,  0.0644,\n",
       "          -0.0039, -0.0753, -0.0228,  0.0302,  0.0367,  0.0650, -0.0032,  0.0337,\n",
       "          -0.0651,  0.0667, -0.0299,  0.0386, -0.0458, -0.0740, -0.0748,  0.0221,\n",
       "           0.0606,  0.0137, -0.0790,  0.0062, -0.0368, -0.0175, -0.0455,  0.0769,\n",
       "          -0.0356, -0.0342,  0.0367, -0.0225,  0.0709,  0.0879, -0.0161,  0.0625,\n",
       "           0.0818,  0.0116, -0.0612,  0.0155,  0.0483, -0.0074, -0.0400, -0.0328,\n",
       "           0.0346,  0.0592,  0.0549,  0.0388,  0.0487, -0.0156, -0.0795, -0.0690,\n",
       "           0.0642,  0.0173,  0.0257, -0.0182,  0.0536,  0.0056,  0.0773,  0.0013],\n",
       "         requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.feed_forward.w_2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0384,  0.0049,  0.0118,  ...,  0.0311, -0.0123, -0.0432],\n",
       "          [ 0.0272, -0.0410,  0.0215,  ...,  0.0155, -0.0396,  0.0173],\n",
       "          [ 0.0415, -0.0306, -0.0061,  ..., -0.0390, -0.0253,  0.0441],\n",
       "          ...,\n",
       "          [-0.0100, -0.0392, -0.0402,  ..., -0.0123,  0.0078,  0.0195],\n",
       "          [ 0.0138,  0.0174,  0.0392,  ..., -0.0412,  0.0068, -0.0060],\n",
       "          [ 0.0092, -0.0078,  0.0222,  ...,  0.0195, -0.0161,  0.0123]],\n",
       "         requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.feed_forward.w_2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0347, -0.0398,  0.0037, -0.0189,  0.0300, -0.0173, -0.0067, -0.0288,\n",
       "          -0.0324,  0.0376, -0.0125,  0.0351,  0.0276, -0.0286,  0.0284, -0.0362,\n",
       "          -0.0329,  0.0173, -0.0400, -0.0050, -0.0324,  0.0225,  0.0273,  0.0207,\n",
       "           0.0373,  0.0341, -0.0010, -0.0170, -0.0407,  0.0001,  0.0253,  0.0245,\n",
       "          -0.0069,  0.0301,  0.0370,  0.0186,  0.0335,  0.0162, -0.0201,  0.0049,\n",
       "          -0.0249,  0.0108,  0.0013, -0.0075, -0.0113, -0.0408, -0.0298, -0.0288,\n",
       "           0.0373, -0.0145, -0.0351, -0.0124,  0.0214, -0.0418, -0.0093, -0.0125,\n",
       "          -0.0209, -0.0333, -0.0150,  0.0048, -0.0348,  0.0316, -0.0057, -0.0431,\n",
       "           0.0324, -0.0070,  0.0026,  0.0308,  0.0173, -0.0028, -0.0090,  0.0030,\n",
       "          -0.0327, -0.0303,  0.0104, -0.0441, -0.0104,  0.0062, -0.0002,  0.0276,\n",
       "          -0.0177, -0.0113, -0.0077, -0.0375,  0.0378, -0.0342, -0.0104,  0.0121,\n",
       "           0.0395, -0.0193, -0.0088,  0.0046, -0.0258,  0.0264,  0.0176, -0.0090,\n",
       "          -0.0221, -0.0219,  0.0103,  0.0062,  0.0170,  0.0035,  0.0129, -0.0292,\n",
       "          -0.0019,  0.0384, -0.0370,  0.0304,  0.0095,  0.0180,  0.0435, -0.0048,\n",
       "           0.0348,  0.0400,  0.0056, -0.0355,  0.0083,  0.0149,  0.0282,  0.0020,\n",
       "           0.0404, -0.0281,  0.0103, -0.0085,  0.0335, -0.0401,  0.0056,  0.0072],\n",
       "         requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.input_sublayer.norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.], requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.input_sublayer.norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.output_sublayer.norm.weight',\n",
       "  Parameter containing:\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.], requires_grad=True)),\n",
       " ('transformer.drugs.transformer.0.output_sublayer.norm.bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n",
       " ('fc.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-2.5435e-02, -4.9001e-03, -1.4550e-02,  2.0067e-02,  1.3538e-02,\n",
       "            3.1318e-02, -3.2520e-02, -6.1041e-03, -4.9332e-02, -3.8017e-02,\n",
       "           -9.7284e-03, -1.5484e-02,  2.5306e-02, -1.1357e-03, -5.4229e-03,\n",
       "           -4.3644e-02,  2.4808e-03,  7.8401e-03,  4.6899e-02,  1.4018e-02,\n",
       "           -3.6113e-02, -3.0725e-02,  2.1732e-02, -4.7013e-02,  3.9634e-02,\n",
       "            4.5256e-02,  2.4808e-02,  1.3109e-02,  2.5819e-03, -4.8621e-02,\n",
       "           -8.7039e-03, -3.4919e-03, -4.7855e-02,  4.5504e-02, -4.8120e-03,\n",
       "            9.4955e-03, -4.6268e-02, -1.5386e-02, -5.6440e-03,  3.4562e-02,\n",
       "           -4.0404e-02, -3.8566e-02,  1.5985e-02, -3.5285e-02,  2.5299e-02,\n",
       "           -4.0969e-02,  5.7439e-03, -4.0833e-02, -3.5091e-02,  1.9264e-02,\n",
       "            1.5061e-02,  1.4316e-02, -4.9236e-02,  2.5682e-02, -3.7479e-02,\n",
       "           -1.7514e-02,  4.2869e-02, -3.3368e-02, -2.0697e-02,  3.4999e-02,\n",
       "            5.3296e-04,  2.7626e-02, -1.2846e-02,  3.9192e-02, -1.2829e-02,\n",
       "            3.4433e-02, -3.7774e-02, -4.6320e-02, -2.9317e-02, -4.0736e-02,\n",
       "           -3.0898e-02, -3.7610e-03, -4.8303e-02,  1.9513e-02, -7.9310e-03,\n",
       "            2.5892e-02,  2.0684e-02, -1.9166e-02, -3.1780e-03, -3.6007e-02,\n",
       "           -4.8148e-02, -1.6438e-02,  5.0831e-02, -1.9747e-02,  6.8069e-03,\n",
       "           -6.3297e-03, -4.1531e-02,  4.6048e-02,  2.2299e-02,  1.5749e-02,\n",
       "           -2.8003e-02, -3.0517e-02,  3.1635e-02,  3.2866e-02,  3.2664e-02,\n",
       "           -8.3443e-03, -3.0859e-02, -3.6065e-02,  1.8047e-02,  3.0291e-02,\n",
       "            2.9763e-02,  1.1322e-02, -6.7830e-03,  2.1335e-02,  3.6666e-02,\n",
       "           -1.5202e-02,  2.9260e-02,  1.4479e-02, -3.8646e-02, -4.0718e-02,\n",
       "            4.7618e-02,  1.7985e-02,  1.7909e-02,  2.4267e-02,  4.8625e-02,\n",
       "            1.6003e-02, -1.7573e-02, -3.3459e-02,  8.2139e-03, -9.7370e-03,\n",
       "           -4.6709e-02, -3.8702e-03, -3.4542e-03, -4.8526e-02, -4.0304e-02,\n",
       "            4.4793e-02,  1.7605e-02, -5.0001e-02,  1.2928e-02, -2.4636e-02,\n",
       "            2.8843e-02,  1.7980e-02, -2.1853e-02,  3.4055e-02,  2.3565e-02,\n",
       "            1.5040e-02,  2.0897e-02,  4.9074e-02,  4.6689e-03,  2.0397e-02,\n",
       "           -2.3269e-02, -3.9440e-02, -5.8128e-03, -2.9578e-03, -3.9373e-02,\n",
       "           -1.4314e-02,  1.3183e-02,  7.6507e-03,  9.0914e-03,  3.5861e-02,\n",
       "           -4.5274e-02,  2.6949e-03, -1.6929e-03,  1.1215e-02, -3.7745e-02,\n",
       "           -7.6183e-03, -4.4984e-02,  3.0234e-02, -1.0972e-03, -5.5389e-03,\n",
       "            9.9075e-03,  2.5045e-02, -4.1114e-03,  2.2720e-02,  3.5760e-02,\n",
       "            1.2744e-02,  4.4174e-03, -9.6160e-03,  2.3774e-02, -3.0962e-02,\n",
       "            2.0267e-02,  1.5130e-02, -3.0246e-02,  2.9159e-02,  4.3128e-02,\n",
       "            4.0872e-02,  2.9680e-02,  4.1518e-02, -7.7740e-03, -1.0037e-02,\n",
       "           -2.5593e-02, -4.9914e-03,  3.5622e-02, -3.5601e-02,  2.5521e-03,\n",
       "            1.2696e-02,  2.0815e-02, -1.6662e-02,  9.0431e-03, -2.3086e-02,\n",
       "            3.9701e-02, -3.7208e-02, -1.8387e-02,  4.8637e-02,  7.6177e-03,\n",
       "           -8.1916e-03,  4.0865e-02, -2.4838e-02,  1.5168e-02,  1.6259e-02,\n",
       "           -4.5819e-02, -1.8243e-02,  2.3381e-02,  8.0804e-03,  1.8179e-02,\n",
       "            2.5833e-02,  1.7630e-02, -3.4931e-02,  3.1405e-02,  3.4435e-02,\n",
       "           -4.0731e-02, -1.4624e-02, -1.6497e-02, -1.6046e-02,  1.4223e-02,\n",
       "            1.3222e-02,  5.9018e-03, -2.4341e-02,  8.0911e-03, -1.1661e-02,\n",
       "           -1.5223e-03,  2.0920e-02,  3.3053e-02,  1.6398e-02,  3.2967e-02,\n",
       "           -3.5873e-03, -7.4506e-03,  4.4426e-02,  3.8451e-02,  1.7078e-02,\n",
       "            4.6292e-02, -1.8329e-02, -5.9884e-03, -3.5315e-02,  3.4223e-02,\n",
       "            4.8803e-02, -2.1066e-02,  1.5223e-02,  1.1786e-03,  1.0963e-02,\n",
       "            5.0081e-03,  2.3833e-02, -4.0122e-03, -4.4885e-02,  2.2538e-02,\n",
       "            3.4427e-02,  2.3672e-02, -1.6407e-02,  5.0427e-02,  4.8491e-02,\n",
       "            3.5237e-02,  4.5430e-03,  4.4100e-02,  1.6305e-02, -9.6598e-05,\n",
       "           -1.0107e-02, -4.3539e-02,  4.1967e-02,  1.0813e-02,  2.5488e-02,\n",
       "            4.7137e-02,  4.7054e-03, -4.9560e-02,  9.9398e-03, -2.0814e-03,\n",
       "           -1.3605e-02, -1.9609e-02, -8.9534e-03, -1.4969e-02, -1.9707e-02,\n",
       "           -1.6645e-02, -2.8088e-02,  4.5961e-02, -1.2203e-02,  1.2443e-03,\n",
       "           -2.5113e-02,  1.5684e-02, -4.2887e-02, -4.4884e-02, -2.4802e-03,\n",
       "            1.6618e-03, -3.7405e-02, -4.7978e-02,  3.7164e-02,  8.7906e-03,\n",
       "           -2.6216e-02, -1.7411e-02, -4.9202e-02, -3.2141e-02, -2.2746e-02,\n",
       "           -1.3789e-02, -5.0781e-02,  4.0785e-02,  3.5770e-02, -3.9388e-02,\n",
       "            6.9991e-03, -2.4939e-02, -7.4975e-03,  3.4525e-02, -8.2740e-03,\n",
       "           -4.6078e-02, -4.4221e-02,  1.9301e-02,  1.5400e-02, -1.4960e-03,\n",
       "           -2.9062e-02,  3.4155e-02,  1.7413e-03,  7.3125e-03,  1.2503e-02,\n",
       "           -3.5508e-02, -1.0829e-02,  1.3087e-02,  4.6840e-02, -8.2099e-03,\n",
       "           -4.2091e-02,  3.8854e-02, -3.7875e-02,  1.7190e-02, -7.4726e-03,\n",
       "           -2.9093e-02, -3.9068e-02,  2.8415e-02, -2.5818e-02,  1.8775e-03,\n",
       "            2.5238e-03,  8.5013e-03,  2.9311e-02,  4.0240e-02, -1.5531e-02,\n",
       "            3.8120e-03,  4.0521e-02,  3.0309e-02, -2.8870e-02,  2.1041e-02,\n",
       "            2.1402e-02,  2.0739e-02, -2.8523e-02,  3.3682e-02,  4.9230e-02,\n",
       "           -2.4563e-03, -4.5278e-02, -1.8235e-02,  3.9615e-04, -2.1782e-02,\n",
       "            4.4482e-02,  3.5900e-02,  8.9846e-03, -4.7617e-02, -3.4036e-02,\n",
       "            2.4742e-02,  1.6541e-02,  4.0703e-02, -4.1650e-02, -1.3438e-02,\n",
       "            3.1735e-02, -1.5299e-02,  2.2743e-02,  1.4773e-02,  7.1161e-03,\n",
       "           -1.9188e-02,  1.6008e-02, -1.4349e-02,  2.5457e-02,  1.2994e-02,\n",
       "            2.2189e-02, -4.9581e-02, -9.3776e-03,  4.3729e-02, -4.8956e-02,\n",
       "           -3.8238e-02, -1.0782e-02, -3.7714e-02,  3.4481e-02,  9.3498e-03,\n",
       "           -3.9978e-02, -6.8509e-03, -2.6327e-02,  2.0616e-02, -3.8305e-03,\n",
       "            3.3239e-02, -2.5747e-02, -4.4589e-02, -6.1215e-03],\n",
       "          [ 2.3550e-02, -3.6542e-02, -1.6153e-02,  2.6732e-02, -3.3465e-02,\n",
       "           -4.7966e-02,  7.2319e-03, -3.4607e-02,  1.8099e-02,  1.9893e-02,\n",
       "            1.8188e-02,  2.6247e-02,  4.5511e-02, -1.7420e-02, -3.6364e-02,\n",
       "            4.1169e-02,  1.2922e-02,  2.8545e-03, -2.9835e-02,  8.1377e-03,\n",
       "            1.4218e-03,  4.5138e-02,  4.0016e-02, -3.3684e-02, -1.5712e-02,\n",
       "            4.7876e-02, -3.0285e-02, -4.9688e-02,  1.0881e-02, -4.1638e-03,\n",
       "           -2.3133e-02, -4.9491e-03,  4.9599e-02, -4.7729e-02,  5.5130e-04,\n",
       "            2.9514e-02, -1.6194e-05,  1.9017e-02, -2.7475e-02,  7.2642e-03,\n",
       "           -4.8805e-02, -1.8852e-02, -4.0272e-02, -1.8803e-02,  3.2844e-03,\n",
       "            3.7567e-02,  3.4797e-02,  4.9079e-02, -4.7988e-02, -2.5186e-02,\n",
       "           -1.5578e-02,  4.0611e-02, -3.3970e-02,  4.0559e-03,  3.2381e-02,\n",
       "           -7.7715e-03, -3.3760e-02,  2.8181e-02, -1.2344e-02, -8.2261e-03,\n",
       "           -1.1015e-02,  4.9148e-02,  3.8853e-02, -1.6706e-02,  2.9892e-02,\n",
       "            3.5320e-02, -4.2327e-02, -3.4253e-02,  2.6724e-02, -3.4398e-02,\n",
       "            4.3783e-02, -4.8035e-02,  9.7151e-03, -4.5941e-02,  1.8226e-02,\n",
       "           -1.5386e-02, -5.0290e-02, -4.8150e-02, -2.0587e-02, -2.9308e-02,\n",
       "            4.5855e-02, -3.8955e-02, -7.0660e-03, -1.1157e-02,  1.0653e-02,\n",
       "           -4.1445e-02,  4.5949e-02,  1.0935e-03, -7.5765e-03,  4.6099e-02,\n",
       "            1.3582e-02, -2.3172e-02, -3.9226e-02, -8.6403e-03, -3.5777e-02,\n",
       "            1.9467e-07,  3.2465e-02, -4.9550e-02,  1.7910e-02, -3.3786e-02,\n",
       "           -4.1174e-02, -2.1109e-02,  4.9502e-02, -1.7655e-02, -1.3321e-02,\n",
       "            2.7671e-03,  2.9974e-02,  3.2237e-02,  4.0465e-02,  3.5259e-03,\n",
       "           -1.6836e-02, -3.1937e-02,  1.3812e-02, -3.2124e-02, -4.8061e-02,\n",
       "           -2.0402e-02, -1.7243e-02, -4.0131e-02, -4.4791e-03, -4.9944e-02,\n",
       "            1.6213e-02, -5.7683e-03,  4.2595e-02, -2.3380e-02, -2.1837e-02,\n",
       "           -1.3149e-02, -2.4540e-02,  3.9247e-02, -2.6623e-02, -1.9139e-02,\n",
       "           -2.9306e-02,  3.7998e-02, -1.2235e-02, -3.8164e-02, -7.7769e-03,\n",
       "            4.3727e-02, -2.1484e-02,  3.4261e-04, -2.0688e-02, -4.9984e-03,\n",
       "           -1.2104e-02,  4.3514e-02,  4.5284e-02, -8.7952e-03,  4.1570e-02,\n",
       "           -6.0622e-03, -8.7470e-04,  3.9799e-02, -4.3242e-02,  4.3674e-02,\n",
       "            4.5352e-02,  3.4421e-02,  1.6965e-02,  5.0052e-02, -1.6636e-02,\n",
       "           -4.6981e-03, -2.9175e-02,  2.5460e-02, -4.7381e-02,  3.4485e-02,\n",
       "           -5.0827e-02, -3.1608e-02,  7.5884e-05, -1.1214e-02, -1.3397e-02,\n",
       "           -3.2855e-02,  4.6694e-02,  3.1070e-02, -3.6722e-03, -4.5378e-02,\n",
       "            3.8868e-02,  3.3268e-02, -6.7077e-03, -2.8450e-02, -3.1277e-02,\n",
       "            2.2123e-02,  2.7415e-03,  5.0297e-03, -1.6445e-02, -4.9538e-02,\n",
       "            4.8703e-02, -4.7681e-02,  4.8183e-03, -3.0571e-02, -2.1019e-02,\n",
       "           -3.1475e-03,  2.5988e-02, -2.5075e-02,  3.1458e-02,  4.8419e-02,\n",
       "           -1.5182e-03, -4.4424e-02,  1.7843e-02,  1.6585e-02, -3.9575e-02,\n",
       "           -4.9641e-02, -1.5876e-02, -1.0999e-02,  2.5925e-02, -5.2517e-03,\n",
       "            3.3279e-02,  3.1568e-02, -2.7825e-02,  4.7944e-02,  1.1045e-02,\n",
       "            3.7863e-02, -4.5676e-02,  1.4394e-02, -1.9612e-02,  4.7680e-02,\n",
       "           -2.8423e-02, -8.6832e-03,  1.3157e-02,  4.6874e-02,  1.6443e-02,\n",
       "           -1.7385e-02,  7.3304e-03,  3.8106e-05,  3.6188e-02,  1.7343e-02,\n",
       "           -2.2312e-02, -2.5492e-02, -1.6310e-02,  1.7169e-02,  4.5855e-02,\n",
       "           -3.8440e-02, -3.7796e-02, -1.3199e-02, -3.9544e-03,  1.5362e-02,\n",
       "            3.1807e-02, -2.6163e-02,  1.9237e-02, -4.0658e-02,  2.2911e-02,\n",
       "           -3.6378e-02,  1.1403e-02,  1.0564e-02, -1.8083e-02, -5.0708e-02,\n",
       "            2.7628e-02,  2.4953e-02, -3.7604e-02,  4.2174e-02,  4.6091e-02,\n",
       "            1.8579e-02,  3.8176e-02, -4.1813e-02,  4.6032e-02,  3.6385e-02,\n",
       "            3.4440e-02, -3.6702e-02,  3.8615e-02,  3.0765e-02, -5.0323e-02,\n",
       "           -6.9952e-03,  2.3456e-02,  4.1114e-02,  2.3641e-03,  4.7336e-02,\n",
       "           -3.8496e-02,  2.6589e-02,  4.1788e-02,  4.9636e-02, -6.7564e-03,\n",
       "            4.6603e-02,  2.2704e-02,  4.7769e-02, -3.6374e-02, -1.5805e-02,\n",
       "           -1.6310e-02,  6.4694e-03,  2.1974e-02, -3.9242e-02, -2.7703e-02,\n",
       "           -1.2213e-02, -8.3848e-03,  2.4763e-02, -4.7551e-02,  3.1169e-03,\n",
       "            3.4422e-02,  1.1550e-02, -2.6658e-02,  4.3881e-03, -3.7445e-02,\n",
       "            3.7644e-03,  7.3623e-03,  2.0830e-03, -3.3889e-02, -3.6734e-02,\n",
       "           -1.5900e-02,  4.2853e-02, -3.3900e-02,  4.9484e-02, -9.3009e-03,\n",
       "           -3.6795e-02, -3.9631e-03,  4.4776e-02, -1.2249e-02,  6.9343e-03,\n",
       "           -2.0469e-02, -1.2867e-02, -1.2311e-02, -2.0450e-02,  1.8979e-02,\n",
       "           -4.9829e-02,  1.1907e-02, -3.4466e-02, -3.8891e-02, -4.4391e-02,\n",
       "            2.7828e-02, -4.8667e-02, -3.0344e-02, -1.8087e-02, -1.2725e-03,\n",
       "            1.4421e-02, -4.1518e-02, -4.2573e-02, -3.2995e-02,  1.5816e-02,\n",
       "            4.7115e-02, -1.8442e-02,  4.5683e-03, -4.3291e-02, -2.0205e-02,\n",
       "            4.1866e-02, -2.4704e-03, -2.2805e-02,  2.4572e-02, -1.2686e-02,\n",
       "            2.7554e-02,  4.6715e-02,  1.4483e-02, -3.4066e-02, -3.9706e-02,\n",
       "           -1.3324e-02, -7.3369e-04,  1.4894e-02,  1.5620e-02, -1.9854e-02,\n",
       "           -4.7980e-03, -4.5358e-02,  7.6644e-03,  2.2211e-02, -7.0641e-03,\n",
       "           -3.4275e-02, -4.0272e-02,  1.2738e-02,  2.3046e-02, -1.6087e-02,\n",
       "            2.4482e-02,  2.9620e-02, -4.9681e-02, -7.3442e-03, -1.3639e-02,\n",
       "           -2.1799e-02,  5.8716e-04, -2.4852e-02, -3.3472e-02, -4.2629e-02,\n",
       "           -2.0923e-02,  4.7697e-02, -4.4637e-02, -2.0880e-02,  2.7000e-02,\n",
       "            4.6945e-02,  3.5285e-02, -5.0669e-02,  3.3483e-02,  2.3170e-02,\n",
       "            4.0289e-02,  1.8065e-02,  4.7477e-02, -1.5596e-02,  3.2954e-02,\n",
       "           -4.5818e-02, -2.0215e-02, -4.5517e-02, -4.7926e-02, -1.4776e-02,\n",
       "           -2.2740e-02, -1.1289e-02, -4.3733e-03,  3.1670e-02]],\n",
       "         requires_grad=True)),\n",
       " ('fc.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0138, -0.0423], requires_grad=True))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = [_ for _ in list(model.named_parameters()) if 'sens_fc' not in _[0]]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F': 0, 'M': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sens_tokenizer.vocabulary.token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sens_tokenizer.convert_tokens_to_indices(['F', 'F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.label_tokenizer.get_vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyhealth.models.base_model.BaseModel.prepare_labels(self, labels: Union[List[str], List[List[str]]], label_tokenizer: pyhealth.tokenizer.Tokenizer) -> torch.Tensor>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prepare_labels(kwargs[model.label_key], model.label_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dataset.get_all_tokens('gender')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model.dataset.get_all_tokens(model.sens_key, remove_duplicates=False, sort=False)\n",
    "Y = model.dataset.get_all_tokens(model.label_key, remove_duplicates=False, sort=False)\n",
    "ttl = model.dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AFAL_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(1674, 256, padding_idx=0)\n",
      "    (procedures): Embedding(571, 256, padding_idx=0)\n",
      "    (drugs): Embedding(178, 256, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (drugs): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (1): Linear(in_features=256, out_features=256, bias=False)\n",
      "              (2): Linear(in_features=256, out_features=256, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: ['roc_auc_macro_ovo']\n",
      "Device: cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = AFAL_Trainer(model=model,device='cuda:0', metrics=['roc_auc_macro_ovo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Batch size: 3200\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f7dc3594f50>\n",
      "Monitor: roc_auc_macro_ovo\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02544403076171875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd30aec11a54444ba6c337ec8b20fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-1 ---\n",
      "loss: 1.3564, sens_loss: 0.8778\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 40.40it/s]\n",
      "--- Eval epoch-0, step-1 ---\n",
      "roc_auc_macro_ovo: 0.5341\n",
      "loss: 0.8216\n",
      "New best roc_auc_macro_ovo score (0.5341) at epoch-0, step-1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021562576293945312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 1 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fad783ebda84382b004455908d6e819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-2 ---\n",
      "loss: 1.1419, sens_loss: 0.8487\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 41.25it/s]\n",
      "--- Eval epoch-1, step-2 ---\n",
      "roc_auc_macro_ovo: 0.5306\n",
      "loss: 0.6481\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02097463607788086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 2 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd47fb380954fa6b3158acb05b4b99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-3 ---\n",
      "loss: 0.9873, sens_loss: 0.8211\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 37.20it/s]\n",
      "--- Eval epoch-2, step-3 ---\n",
      "roc_auc_macro_ovo: 0.5293\n",
      "loss: 0.5358\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020493268966674805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 3 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3a54f9a02345388250842647092880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-4 ---\n",
      "loss: 0.9896, sens_loss: 0.8133\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 34.77it/s]\n",
      "--- Eval epoch-3, step-4 ---\n",
      "roc_auc_macro_ovo: 0.5319\n",
      "loss: 0.4694\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02934741973876953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 4 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f3130a51284fe0aedf2c4753273621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-5 ---\n",
      "loss: 0.8430, sens_loss: 0.7930\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 38.15it/s]\n",
      "--- Eval epoch-4, step-5 ---\n",
      "roc_auc_macro_ovo: 0.5346\n",
      "loss: 0.4359\n",
      "New best roc_auc_macro_ovo score (0.5346) at epoch-4, step-5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019559621810913086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 5 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668fe6162ce840e19d5a3534661c7033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-5, step-6 ---\n",
      "loss: 0.6908, sens_loss: 0.7749\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 42.60it/s]\n",
      "--- Eval epoch-5, step-6 ---\n",
      "roc_auc_macro_ovo: 0.5380\n",
      "loss: 0.4243\n",
      "New best roc_auc_macro_ovo score (0.5380) at epoch-5, step-6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018861055374145508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 6 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb52eec60e804e3798d4f476ebbf7dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-6, step-7 ---\n",
      "loss: 0.6253, sens_loss: 0.7911\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 37.92it/s]\n",
      "--- Eval epoch-6, step-7 ---\n",
      "roc_auc_macro_ovo: 0.5346\n",
      "loss: 0.4262\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019789695739746094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 7 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd240ada6af9446e80a3ba2d634ec0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-7, step-8 ---\n",
      "loss: 0.6121, sens_loss: 0.7935\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 37.84it/s]\n",
      "--- Eval epoch-7, step-8 ---\n",
      "roc_auc_macro_ovo: 0.5346\n",
      "loss: 0.4363\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019523143768310547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 8 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942afe7a10794d40b5c54dc86e44d4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-8, step-9 ---\n",
      "loss: 0.5496, sens_loss: 0.8044\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 37.66it/s]\n",
      "--- Eval epoch-8, step-9 ---\n",
      "roc_auc_macro_ovo: 0.5328\n",
      "loss: 0.4508\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02026510238647461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 9 / 10",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24afa63d0a324e6a95b5bd273096aaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 / 10:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-9, step-10 ---\n",
      "loss: 0.5595, sens_loss: 0.7881\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 28.01it/s]\n",
      "--- Eval epoch-9, step-10 ---\n",
      "roc_auc_macro_ovo: 0.5354\n",
      "loss: 0.4673\n",
      "Loaded best model\n"
     ]
    }
   ],
   "source": [
    "#trainer = Trainer(model=model,device='cuda:0')\n",
    "trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=10,\n",
    "    monitor='roc_auc_macro_ovo',#\"pr_auc_samples\",\n",
    "    #adv_rate = 10.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 14.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc_macro_ovo': 0.5536137440758293, 'loss': 0.41003865003585815}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import fair_check, evaluate\n",
    "evaluate(trainer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 141.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape torch.Size([26, 2])\n",
      "sens_logits.shape torch.Size([26, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc_macro_ovo': 0.6, 'loss': 1.501630187034607}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import fair_check, evaluate\n",
    "evaluate(trainer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 177.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.32962962962962955}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import fair_check\n",
    "fair_check(trainer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 188.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.03872053872053871}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import fair_check\n",
    "fair_check(trainer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020940303802490234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba285a60a4af4fe895364e411c8958d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0005\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 178.32it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3443\n",
      "loss: 0.6593\n",
      "New best pr_auc_samples score (0.3443) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 136.75it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 144.21it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020702838897705078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03016b7c4147479bb6ae3fe61af22ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0498\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 174.91it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3190\n",
      "loss: 0.7097\n",
      "New best pr_auc_samples score (0.3190) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 138.46it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 145.65it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021233797073364258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aa117e988a4770a267239a6dc55ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0407\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 182.23it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3387\n",
      "loss: 0.7055\n",
      "New best pr_auc_samples score (0.3387) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 137.42it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 146.96it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02138996124267578,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15939d698f7d4b748c69bac514d93ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0234\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 181.27it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3554\n",
      "loss: 0.6835\n",
      "New best pr_auc_samples score (0.3554) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 140.01it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 147.00it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021580219268798828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efa94557b9c49b1bef76258eb55d6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0253\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 133.28it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3074\n",
      "loss: 0.7304\n",
      "New best pr_auc_samples score (0.3074) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 140.33it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 145.83it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021781444549560547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d58a3c063484b68bf09bbf59f46eb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0102\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 182.16it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3488\n",
      "loss: 0.7025\n",
      "New best pr_auc_samples score (0.3488) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 139.56it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 146.67it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021641254425048828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25335ea13e3a416a811c1f5f84ada504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 0.9957\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 178.63it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3522\n",
      "loss: 0.6790\n",
      "New best pr_auc_samples score (0.3522) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 139.16it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 144.41it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02232217788696289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190d739402f44bb99e5f16e36b360b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0567\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 179.66it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3192\n",
      "loss: 0.6907\n",
      "New best pr_auc_samples score (0.3192) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 137.68it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 145.85it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022925615310668945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e4a7fa069d42f9852e02ae25aa5da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0248\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 142.65it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3222\n",
      "loss: 0.6956\n",
      "New best pr_auc_samples score (0.3222) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 138.91it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 143.96it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02178812026977539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d983ded053f44410b5cad8a5b0151f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0544\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 180.34it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3168\n",
      "loss: 0.7513\n",
      "New best pr_auc_samples score (0.3168) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 138.00it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 144.97it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021289348602294922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490d89cebec647fc8a4b249c5cffa580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0383\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 176.64it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3055\n",
      "loss: 0.7355\n",
      "New best pr_auc_samples score (0.3055) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 139.61it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 144.11it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023786544799804688,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b8fc747871489ab347a855021c0c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0025\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 181.48it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3541\n",
      "loss: 0.6986\n",
      "New best pr_auc_samples score (0.3541) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 138.77it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 146.24it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02114105224609375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde0e8ed534c47cabca6cfaa511714e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0599\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 172.71it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3232\n",
      "loss: 0.6860\n",
      "New best pr_auc_samples score (0.3232) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 141.62it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 147.57it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02072429656982422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89559f52b3a642a894aced82aa332601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0018\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 181.79it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3460\n",
      "loss: 0.6723\n",
      "New best pr_auc_samples score (0.3460) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 138.90it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 146.83it/s]\n",
      "Adv_Transformer(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(917, 128, padding_idx=0)\n",
      "    (procedures): Embedding(305, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (1): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (2): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (output_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (attention): Attention()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=165, bias=True)\n",
      "  (sens_fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda:0\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9ebf841950>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021519184112548828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Epoch 0 / 1",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee088bded8945c291056a7ec4b86c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 1.0493\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 181.97it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc_samples: 0.3101\n",
      "loss: 0.7444\n",
      "New best pr_auc_samples score (0.3101) at epoch-0, step-7\n",
      "Loaded best model\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 139.34it/s]\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 145.25it/s]\n"
     ]
    }
   ],
   "source": [
    "final_result = {}\n",
    "for subsample_rate in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    final_result[subsample_rate] = {'auc':[], 'auc_diff': []}\n",
    "    for _ in range(3):\n",
    "        model = Adv_Transformer(\n",
    "            dataset=mimic3sample,# the dataset should provide sens feat\n",
    "            feature_keys=[\"conditions\", \"procedures\"], # the model should specify the sens feat\n",
    "            label_key=\"drugs\",\n",
    "            sens_key='gender',\n",
    "            mode=\"multilabel\",\n",
    "            sens_mode = 'multiclass',\n",
    "            # the model should provide sensitive attribute   \n",
    "        )\n",
    "        trainer = Trainer(model=model,device='cuda:0')\n",
    "\n",
    "        #trainer = Trainer(model=model,device='cuda:0')\n",
    "        trainer.train(\n",
    "            train_dataloader= get_dataloader(subsample_subdataset(train_ds, 'gender', subsample_rate), batch_size=32, shuffle=True),\n",
    "            val_dataloader=val_loader,\n",
    "            epochs=1,\n",
    "            monitor=\"pr_auc_samples\",\n",
    "            #adv_rate = subsample_rate,\n",
    "        )\n",
    "        \n",
    "        final_result[subsample_rate]['auc'] += [evaluate(trainer, test_loader)['pr_auc_samples']]\n",
    "        final_result[subsample_rate]['auc_diff'] += [fair_check(trainer, test_loader)['pr_auc_samples']]\n",
    "\n",
    "    final_result[subsample_rate]['auc'] = {'mean': np.mean(final_result[subsample_rate]['auc']), 'std': np.std(final_result[subsample_rate]['auc'])}\n",
    "    final_result[subsample_rate]['auc_diff'] = {'mean': np.mean(final_result[subsample_rate]['auc_diff']), 'std': np.std(final_result[subsample_rate]['auc_diff'])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.30370512899526597, 'std': 0.026583837395578767}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(final_result[0.0]['auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.09379999239964172, 'std': 0.020344631310541664}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(final_result[0.0]['auc_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_result, 'adv_test_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = torch.load('subsample_test_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_list = [final_result[adv_rate]['auc']['mean'] for adv_rate in final_result.keys()]\n",
    "auc_std_list = [final_result[adv_rate]['auc']['std'] for adv_rate in final_result.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(final_result.keys()), auc_list)\n",
    "plt.errorbar(final_result.keys(), auc_list, yerr=auc_std_list, label='both limits (default)')\n",
    "#plt.xscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_diff_list = [final_result[adv_rate]['auc_diff']['mean'] for adv_rate in final_result.keys()]\n",
    "auc__diff_std_list = [final_result[adv_rate]['auc_diff']['std'] for adv_rate in final_result.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(final_result.keys()), auc_diff_list)\n",
    "plt.errorbar(final_result.keys(), auc_diff_list, yerr=auc__diff_std_list, label='both limits (default)')\n",
    "#plt.xscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.0, 0.1, 1.0, 10.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = subsample_subdataset(train_ds)\n",
    "train_loader = get_dataloader(train_ds, batch_size=32, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "581cff410c649e3123de6f78b6d59996374b29e4c31d235b1d5702c2341175c9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('graph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
